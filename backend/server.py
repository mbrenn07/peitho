from flask import request
from collections import Counter
import assemblyai as aai
import yt_dlp
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import nltk
import string
import re
import base64
import time
from urllib.parse import urlparse, parse_qs
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
import requests

app = Flask(__name__)
cors = CORS(app)

DIALOG_CLASSIFIER_URL = os.getenv("DIALOG_CLASSIFIER_URL")
SENTIMENT_URL = os.getenv("SENTIMENT_URL")
HF_TOKEN = os.getenv("HF_TOKEN")

hf_headers = {
    "Accept": "application/json",
    "Authorization": f"Bearer {HF_TOKEN}",
    "Content-Type": "application/json"
}

uri = os.getenv("MONGO_URI")
client = MongoClient(uri, server_api=ServerApi('1'))
db = client['user-paths']
videos_collection = db['videos']


def convert_cookies(cookies):
    cookie_file_lines = [
        "# Netscape HTTP Cookie File",
        "# This file was generated by yt-dlp",
        "# HttpOnly, Secure flags can be included in the last column",
        ""
    ]

    for cookie in cookies:
        domain = cookie['domain']
        path = cookie.get('path', '/')
        secure = 'TRUE' if cookie.get('secure', False) else 'FALSE'
        expiry = cookie.get('expirationDate', None)
        name = cookie['name']
        value = cookie['value']

        if name == "SID" or name == "HSID" or name == "APISID" or name == "LOGIN_INFO" or "ST" in name or name == "SIDCC":
            continue

        if expiry is not None:
            expiry = int(expiry)
        else:
            expiry = int(time.time()) + 31536000

        domain_specified = 'TRUE' if domain.startswith('.') else 'FALSE'
        cookie_line = f"{domain}\t{domain_specified}\t{path}\t{secure}\t{expiry}\t{name}\t{value}"
        cookie_file_lines.append(cookie_line)

    return "\n".join(cookie_file_lines)


def save_cookies_to_file(cookies, filename="cookies.txt"):
    cookie_file_content = convert_cookies(cookies)

    with open(filename, "w", encoding="utf-8") as f:
        f.write(cookie_file_content)


class UtteranceSeparator:
    def __init__(self):
        nltk.download('punkt')
        nltk.download('punkt_tab')
        nltk.download('cmudict')
        self.cmu_dict = nltk.corpus.cmudict.dict()

    def preprocess_abbreviations_extended(self, text):
        # Dictionary of common abbreviations and their temporary replacements
        common_abbrev = {
            # U.S.A. -> USA
            r'(?<![\w.])((?:[A-Z]\.[A-Z]\.)+(?:[A-Z])?\.?)': lambda m: m.group(0).replace('.', ''),
            # Mr. -> Mr
            r'(?<![\w.])(Mr\.|Mrs\.|Dr\.|Prof\.)': lambda m: m.group(0).replace('.', ''),
            # a.m. -> am
            r'(?<![\w.])(a\.m\.|p\.m\.)': lambda m: m.group(0).replace('.', ''),
        }

        processed_text = text
        replacements = {}

        for pattern, replacement_func in common_abbrev.items():
            def replace_match(match):
                original = match.group(0)
                replaced = replacement_func(match)
                replacements[replaced] = original
                return replaced

            processed_text = re.sub(pattern, replace_match, processed_text)

        return processed_text, replacements

    def clean_text(self, text):
        valid_chars = string.ascii_letters + string.digits + \
            "".join([" ", ".", "?", "!", ",", "\'"])
        text = "".join([i if i in valid_chars else " " for i in text])
        while "  " in text:
            text = text.replace("  ", " ")
        text = text.strip()
        return text

    def rebuild_utterance_data(self, data):
        speakers_list = data['speakers']
        speaker_map = dict([(j, i+1)
                           for i, j in enumerate(sorted(speakers_list))])
        speakers_list = [speaker_map[i] for i in speakers_list]
        all_utterances = []
        for i in range(len(data['utterances'])):
            text = data['utterances'][i]['text']
            speaker = speaker_map[data['utterances'][i]['speaker']]
            label = data['utterances'][i]['label']

            cleaned_text = self.clean_text(text)
            cleaned_data, replacements = self.preprocess_abbreviations_extended(
                cleaned_text)
            utterances = nltk.sent_tokenize(cleaned_data)
            for j in range(len(utterances)):
                all_utterances.append({
                    'speaker': speaker,
                    'text': utterances[j],
                    'start': None,
                    'end': None,
                    'label': label
                })
        output_data = {
            'utterances': all_utterances,
            'speakers': speakers_list
        }
        return output_data


utterance_seperator = UtteranceSeparator()


@app.route("/get_all_videos_for_speaker", methods=["POST"])
def get_all_videos_for_speaker():
    speaker = request.get_json()["speaker"]
    results = videos_collection.find({"speakers": speaker})

    overall_label_counts = Counter()
    overall_sentiment_counts = Counter()
    videos_data = []

    for doc in results:
        utterances = doc.get("utterances", [])

        label_counts = Counter()
        sentiment_counts = Counter()

        for u in utterances:
            labels = u.get("labels", [])
            sentiment = u.get("sentiment", "unknown")[0]

            for label in labels:
                label_counts[label] += 1
                overall_label_counts[label] += 1

            sentiment_counts[sentiment] += 1
            overall_sentiment_counts[sentiment] += 1

        videos_data.append({
            "thumbnail": doc.get("thumbnail", ""),
            "title": doc.get("title", "Unknown Title"),
            "date": doc.get("date", "Unknown Date"),
            "description": doc.get('description', 'No description available'),
            "overallLabel": dict(label_counts),
            "overallSentiment": dict(sentiment_counts)
        })

    return jsonify({
        "overallLabel": dict(overall_label_counts),
        "overallSentiment": dict(overall_sentiment_counts),
        "videos": videos_data
    })


@app.route("/autofill_speakers")
def autofill_speakers():
    return videos_collection.distinct("speakers")


@app.route("/speaker_vote", methods=["POST"])
def speaker_vote():

    vote_diff = request.get_json()["vote_diff"]
    raw_url = request.get_json()["url"]
    parsed_url = urlparse(raw_url)
    query_params = parse_qs(parsed_url.query)
    video_id = query_params.get("v", [None])[0]
    if not video_id:
        return {"error": "Invalid YouTube URL"}, 400

    url = f"{parsed_url.scheme}://{parsed_url.netloc}/watch?v={video_id}"

    video = videos_collection.find_one({"url": url})

    if not video:
        return {"error": "Video not yet cached"}, 400

    updated_video = {"votes": video.get("votes", None)}

    if updated_video["votes"]:
        if vote_diff["add"]["name"] in updated_video["votes"][vote_diff["add"]["index"]]:
            updated_video["votes"][vote_diff["add"]
                                   ["index"]][vote_diff["add"]["name"]] = updated_video["votes"][vote_diff["add"]["index"]][vote_diff["add"]["name"]] + 1
        else:
            updated_video["votes"][vote_diff["add"]
                                   ["index"]][vote_diff["add"]["name"]] = 1

        if "sub" in vote_diff:
            if vote_diff["sub"]["name"] in updated_video["votes"][vote_diff["sub"]["index"]]:
                updated_video["votes"][vote_diff["sub"]
                                       ["index"]][vote_diff["sub"]["name"]] = updated_video["votes"][vote_diff["sub"]["index"]][vote_diff["sub"]["name"]] - 1
            else:
                updated_video["votes"][vote_diff["sub"]
                                       ["index"]][vote_diff["sub"]["name"]] = 1

    else:
        new_votes = {}
        for i, speaker in enumerate(video["speakers"]):
            new_votes[str(i)] = {}

        new_votes[vote_diff["add"]
                  ["index"]][vote_diff["add"]["name"]] = 1

        updated_video["votes"] = new_votes

    returned_speakers = video["speakers"]

    sorted_keys = sorted(
        updated_video["votes"].keys(), key=lambda x: int(x))
    for int_key, index in enumerate(sorted_keys):
        key = str(int_key)
        if updated_video["votes"][key]:
            best_option = None
            best_option_count = -1
            for option in updated_video["votes"][key]:
                if updated_video["votes"][key][option] > best_option_count:
                    best_option = option
                    best_option_count = updated_video["votes"][key][option]

            best_option = best_option.split()
            best_option = " ".join([word.capitalize()
                                    for word in best_option])

            returned_speakers[int(index)] = best_option

    updated_video["speakers"] = returned_speakers

    print(updated_video)

    result = videos_collection.update_one(
        {"url": url},  # Find by URL (or any other unique identifier)
        {"$set": updated_video}  # Update the fields with new data
    )

    if result.modified_count > 0:
        return {"message": "Video updated successfully!"}, 200
    else:
        return {"message": "No changes made to the video."}, 200


@app.route("/utterance_vote", methods=["POST"])
def utterance_vote():
    data = request.get_json()
    raw_url = data.get("url")
    vote = data.get("vote")

    if not raw_url or not vote:
        return {"error": "Missing required data"}, 400

    parsed_url = urlparse(raw_url)
    query_params = parse_qs(parsed_url.query)
    video_id = query_params.get("v", [None])[0]
    if not video_id:
        return {"error": "Invalid YouTube URL"}, 400

    url = f"{parsed_url.scheme}://{parsed_url.netloc}/watch?v={video_id}"
    video = videos_collection.find_one({"url": url})
    if not video:
        return {"error": "Video not yet cached"}, 400

    utterances = video.get("utterances", [])
    utt_index = vote.get("index")

    if utt_index is None or utt_index >= len(utterances):
        return {"error": "Invalid utterance index"}, 400

    utterance = utterances[utt_index]
    utterance.setdefault("label_votes", {})
    utterance.setdefault("sentiment_votes", {})
    labels = utterance["labels"]

    if "label" in vote:
        label_vote = vote["label"]

        for action in ["add", "sub"]:
            label_info = label_vote.get(action)
            if not label_info:
                continue

            label_str = label_info.get("label")
            label_index = str(label_info.get("index"))

            if label_index is None or label_str is None:
                return {"error": f"Missing label or index in '{action}'"}, 400

            label_votes = utterance["label_votes"]
            index_votes = label_votes.setdefault(label_index, {})
            current_count = index_votes.get(label_str, 0)

            if action == "add":
                index_votes[label_str] = current_count + 1
            elif action == "sub":
                index_votes[label_str] = max(current_count - 1, 0)

        for label_index, index_votes in utterance["label_votes"].items():
            best_label = max(index_votes.items(), key=lambda item: item[1])[0]
            labels[int(label_index)] = best_label

    if "sentiment" in vote:
        sentiment_votes = utterance["sentiment_votes"]

        def update_sentiment_votes(action):
            section = vote["sentiment"].get(action)
            if not section:
                return
            label = section.get("label")
            if not label:
                return
            current_count = sentiment_votes.get(label, 0)
            if action == "add":
                sentiment_votes[label] = current_count + 1
            elif action == "sub":
                sentiment_votes[label] = max(current_count - 1, 0)

        update_sentiment_votes("add")
        update_sentiment_votes("sub")

        if sentiment_votes:
            utterance["sentiment"] = [max(
                sentiment_votes.items(), key=lambda x: x[1])[0]]

    print(utterance["label_votes"])

    result = videos_collection.update_one(
        {"url": url},
        {"$set": {"utterances": utterances}}
    )

    if result.modified_count > 0:
        return {"message": "Utterance updated successfully!"}, 200
    else:
        return {"message": "No changes made to the utterance."}, 200


@app.route("/process_transcript", methods=["POST"])
def process_transcript():
    raw_url = request.get_json()["url"]
    parsed_url = urlparse(raw_url)
    query_params = parse_qs(parsed_url.query)
    video_id = query_params.get("v", [None])[0]
    save_cookies_to_file(request.get_json()["cookies"])
    if not video_id:
        return {"error": "Invalid YouTube URL"}, 400

    url = f"{parsed_url.scheme}://{parsed_url.netloc}/watch?v={video_id}"

    existing_video = videos_collection.find_one({"url": url})
    if existing_video:
        return {"utterances": existing_video["utterances"], "speakers": existing_video["speakers"], "thumbnail": existing_video.get("thumbnail", ""), "title": existing_video.get("title", "Unknown Title"), "date": existing_video.get("date", "Unknown Date"), "description": existing_video.get('description', 'No description available')}
    options = {
        'format': 'bestaudio[ext=webm]/bestaudio/best',
        'outtmpl': 'videos/%(title)s.%(ext)s',
        'cookiefile': 'cookies.txt',
    }

    with yt_dlp.YoutubeDL(options) as ydl:
        info_dict = ydl.extract_info(url, download=True)
        file_path = ydl.prepare_filename(info_dict)

        # Get the title and upload date
        title = info_dict.get('title', 'Unknown Title')
        description = info_dict.get('description', 'No description available')
        upload_date_raw = info_dict.get('upload_date')  # Format: YYYYMMDD

        if upload_date_raw and len(upload_date_raw) == 8:
            upload_date = f"{upload_date_raw[4:6]}/{upload_date_raw[6:]}/{upload_date_raw[:4]}"
        else:
            upload_date = "Unknown Date"

        # Get the thumbnail URL from the metadata
        thumbnail_url = info_dict.get('thumbnail')

        base64_thumbnail = None
        if thumbnail_url:
            response = requests.get(thumbnail_url)
            content_type = response.headers['Content-Type']
            base64data = base64.b64encode(
                response.content).decode('utf-8')
            base64_thumbnail = f"data:{content_type};base64,{base64data}"

    aai.settings.api_key = os.getenv("ASSEMBLY_AI_API")
    config = aai.TranscriptionConfig(speaker_labels=True)
    transcriber = aai.Transcriber()
    transcript = transcriber.transcribe(file_path, config=config)

    raw_utterances = [
        {
            "speaker": utterance.speaker,
            "text": utterance.text,
            "start": utterance.start,
            "end": utterance.end
        }
        for utterance in transcript.utterances
    ]

    speakers_set = {utt["speaker"] for utt in raw_utterances}

    # Assign empty label and run through rebuild logic
    for utt in raw_utterances:
        utt["label"] = 0  # temporary placeholder

    separated_data = utterance_seperator.rebuild_utterance_data({
        "utterances": raw_utterances,
        "speakers": list(speakers_set)
    })

    word_index = 0
    for utterance in separated_data["utterances"]:
        utterance_length = len(utterance["text"].split())

        utterance["start"] = transcript.words[word_index].start
        word_index = word_index + utterance_length - 1
        utterance["end"] = transcript.words[word_index].end
        word_index = word_index + 1

    print(separated_data)
    # Call classification APIs for each utterance
    utterance_texts = [utt["text"] for utt in separated_data["utterances"]]
    dialog_preds = call_dialog_classifier_batch(separated_data["utterances"])
    print(dialog_preds)
    sentiment_preds = call_sentiment_classifier_batch(utterance_texts)
    print(sentiment_preds)

    for i, utterance in enumerate(separated_data["utterances"]):
        utterance["labels"] = [label_info["label"]
                               for label_info in dialog_preds[i]]
        utterance["sentiment"] = [sentiment_preds[i]]

    modified_speakers = [
        f"Speaker {index + 1}" for index, _ in enumerate(separated_data['speakers'])]

    video_data = {
        "url": url,
        "utterances": separated_data["utterances"],
        "speakers": modified_speakers,
        "thumbnail": base64_thumbnail,
        "title": title,
        "date": upload_date,
        "description": description,
    }

    print(video_data)

    videos_collection.insert_one(video_data)

    try:
        os.remove(file_path)
    except Exception as e:
        print(f"An error occurred: {e}")

    try:
        os.remove("cookies.txt")
    except Exception as e:
        print(f"An error occurred: {e}")

    return {"utterances": separated_data["utterances"], "speakers": modified_speakers, "thumbnail": base64_thumbnail, "title": title, "date": upload_date, "description": description}


def call_dialog_classifier_batch(utterance_list, context_window=2):
    context_encoded_inputs = []

    for i, curr_utt in enumerate(utterance_list):
        current_text = curr_utt["text"]
        current_speaker = curr_utt["speaker"]

        # Get previous utterances from same video, regardless of speaker
        prev = []
        for j in range(i - context_window, i):
            if j >= 0:
                speaker = utterance_list[j]["speaker"]
                text = utterance_list[j]["text"]
                prev.append(f"{speaker}: {text}")

        context_str = " [SEP] ".join(prev)
        if context_str:
            full_text = f"{context_str} [CURRENT SPEAKER: {current_speaker}] {current_text}"
        else:
            full_text = f"[CURRENT SPEAKER: {current_speaker}] {current_text}"

        context_encoded_inputs.append(full_text)

    data = {"inputs": context_encoded_inputs}
    response = requests.post(DIALOG_CLASSIFIER_URL,
                             headers=hf_headers, json=data)

    print("Dialog classifier batch response status:", response.status_code)

    if response.status_code == 200:
        try:
            predictions = response.json()
            if "predictions" in predictions:
                # multilabel
                return [p["predicted_labels"] for p in predictions["predictions"]]
        except Exception as e:
            print("Error parsing dialog classifier response:", e)

    print("Dialog classification failed or format unexpected")
    # return empty multilabel predictions if fail
    return [[] for _ in utterance_list]


def call_sentiment_classifier_batch(text_list):
    data = {"inputs": text_list}
    response = requests.post(SENTIMENT_URL, headers=hf_headers, json=data)

    if response.status_code == 200:
        try:
            predictions = response.json()
            if isinstance(predictions, list) and "label" in predictions[0]:
                return [p["label"] for p in predictions]
            elif isinstance(predictions, dict) and "predictions" in predictions:
                return [p["label"] for p in predictions["predictions"]]
            raise ValueError(
                f"Unexpected sentiment classifier format: {predictions}")
        except Exception as e:
            print("Error parsing sentiment classifier response:", e)
            return ["neutral"] * len(text_list)
    else:
        print(f"Sentiment classification failed: {response.text}")
        return ["neutral"] * len(text_list)
