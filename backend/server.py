import assemblyai as aai
import yt_dlp
from flask import Flask, request
from flask_cors import CORS
import random
import os
import nltk
import string
import re
import json
import time
from urllib.parse import urlparse, parse_qs
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
import requests

app = Flask(__name__)
cors = CORS(app)

DIALOG_CLASSIFIER_URL = os.getenv("DIALOG_CLASSIFIER_URL")
SENTIMENT_URL = os.getenv("SENTIMENT_URL")
HF_TOKEN = os.getenv("HF_TOKEN")

hf_headers = {
    "Accept": "application/json",
    "Authorization": f"Bearer {HF_TOKEN}",
    "Content-Type": "application/json"
}

uri = os.getenv("MONGO_URI")
client = MongoClient(uri, server_api=ServerApi('1'))
db = client['user-paths']
videos_collection = db['videos']


def convert_cookies(cookies):
    cookie_file_lines = [
        "# Netscape HTTP Cookie File",
        "# This file was generated by yt-dlp",
        "# HttpOnly, Secure flags can be included in the last column",
        ""
    ]

    for cookie in cookies:
        domain = cookie['domain']
        path = cookie.get('path', '/')
        secure = 'TRUE' if cookie.get('secure', False) else 'FALSE'
        expiry = cookie.get('expirationDate', None)
        name = cookie['name']
        value = cookie['value']

        if name == "SID" or name == "HSID" or name == "APISID" or name == "LOGIN_INFO" or "ST" in name or name == "SIDCC":
            continue

        if expiry is not None:
            expiry = int(expiry)
        else:
            expiry = int(time.time()) + 31536000

        domain_specified = 'TRUE' if domain.startswith('.') else 'FALSE'
        cookie_line = f"{domain}\t{domain_specified}\t{path}\t{secure}\t{expiry}\t{name}\t{value}"
        cookie_file_lines.append(cookie_line)

    return "\n".join(cookie_file_lines)


def save_cookies_to_file(cookies, filename="cookies.txt"):
    cookie_file_content = convert_cookies(cookies)

    with open(filename, "w", encoding="utf-8") as f:
        f.write(cookie_file_content)


class UtteranceSeparator:
    def __init__(self):
        nltk.download('punkt')
        nltk.download('punkt_tab')
        nltk.download('cmudict')
        self.cmu_dict = nltk.corpus.cmudict.dict()

    def preprocess_abbreviations_extended(self, text):
        # Dictionary of common abbreviations and their temporary replacements
        common_abbrev = {
            # U.S.A. -> USA
            r'(?<![\w.])((?:[A-Z]\.[A-Z]\.)+(?:[A-Z])?\.?)': lambda m: m.group(0).replace('.', ''),
            # Mr. -> Mr
            r'(?<![\w.])(Mr\.|Mrs\.|Dr\.|Prof\.)': lambda m: m.group(0).replace('.', ''),
            # a.m. -> am
            r'(?<![\w.])(a\.m\.|p\.m\.)': lambda m: m.group(0).replace('.', ''),
        }

        processed_text = text
        replacements = {}

        for pattern, replacement_func in common_abbrev.items():
            def replace_match(match):
                original = match.group(0)
                replaced = replacement_func(match)
                replacements[replaced] = original
                return replaced

            processed_text = re.sub(pattern, replace_match, processed_text)

        return processed_text, replacements

    def clean_text(self, text):
        valid_chars = string.ascii_letters + string.digits + \
            "".join([" ", ".", "?", "!", ",", "\'"])
        text = "".join([i if i in valid_chars else " " for i in text])
        while "  " in text:
            text = text.replace("  ", " ")
        text = text.strip()
        return text

    def rebuild_utterance_data(self, data):
        speakers_list = data['speakers']
        speaker_map = dict([(j, i+1)
                           for i, j in enumerate(sorted(speakers_list))])
        speakers_list = [speaker_map[i] for i in speakers_list]
        all_utterances = []
        for i in range(len(data['utterances'])):
            text = data['utterances'][i]['text']
            speaker = speaker_map[data['utterances'][i]['speaker']]
            label = data['utterances'][i]['label']

            cleaned_text = self.clean_text(text)
            cleaned_data, replacements = self.preprocess_abbreviations_extended(
                cleaned_text)
            utterances = nltk.sent_tokenize(cleaned_data)
            for j in range(len(utterances)):
                all_utterances.append({
                    'speaker': speaker,
                    'text': utterances[j],
                    'start': None,
                    'end': None,
                    'label': label
                })
        output_data = {
            'utterances': all_utterances,
            'speakers': speakers_list
        }
        return output_data


utterance_seperator = UtteranceSeparator()


@app.route("/speaker_vote", methods=["POST"])
def speaker_vote():
    vote_diff = request.get_json()["vote_diff"]
    raw_url = request.get_json()["url"]
    parsed_url = urlparse(raw_url)
    query_params = parse_qs(parsed_url.query)
    video_id = query_params.get("v", [None])[0]
    if not video_id:
        return {"error": "Invalid YouTube URL"}, 400

    url = f"{parsed_url.scheme}://{parsed_url.netloc}/watch?v={video_id}"

    video = videos_collection.find_one({"url": url})

    if not video:
        return {"error": "Video not yet cached"}, 400

    updated_video = {"votes": video.get("votes", None)}

    if updated_video["votes"]:
        if vote_diff["add"]["name"] in updated_video["votes"][vote_diff["add"]["index"]]:
            updated_video["votes"][vote_diff["add"]
                                   ["index"]][vote_diff["add"]["name"]] = updated_video["votes"][vote_diff["add"]["index"]][vote_diff["add"]["name"]] + 1
        else:
            updated_video["votes"][vote_diff["add"]
                                   ["index"]][vote_diff["add"]["name"]] = 1

        if "sub" in vote_diff:
            if vote_diff["sub"]["name"] in updated_video["votes"][vote_diff["sub"]["index"]]:
                updated_video["votes"][vote_diff["sub"]
                                       ["index"]][vote_diff["sub"]["name"]] = updated_video["votes"][vote_diff["sub"]["index"]][vote_diff["sub"]["name"]] - 1
            else:
                updated_video["votes"][vote_diff["sub"]
                                       ["index"]][vote_diff["sub"]["name"]] = 1

    else:
        new_votes = {}
        for i, speaker in enumerate(video["speakers"]):
            new_votes[str(i)] = {}

        new_votes[vote_diff["add"]
                  ["index"]][vote_diff["add"]["name"]] = 1

        updated_video["votes"] = new_votes

    print(updated_video)

    result = videos_collection.update_one(
        {"url": url},  # Find by URL (or any other unique identifier)
        {"$set": updated_video}  # Update the fields with new data
    )

    if result.modified_count > 0:
        return {"message": "Video updated successfully!"}, 200
    else:
        return {"message": "No changes made to the video."}, 200


@app.route("/process_transcript", methods=["POST"])
def process_transcript():
    raw_url = request.get_json()["url"]
    parsed_url = urlparse(raw_url)
    query_params = parse_qs(parsed_url.query)
    video_id = query_params.get("v", [None])[0]
    save_cookies_to_file(request.get_json()["cookies"])
    if not video_id:
        return {"error": "Invalid YouTube URL"}, 400

    url = f"{parsed_url.scheme}://{parsed_url.netloc}/watch?v={video_id}"

    existing_video = videos_collection.find_one({"url": url})
    if existing_video:
        if "votes" in existing_video:
            returned_speakers = existing_video["speakers"]

            sorted_keys = sorted(
                existing_video["votes"].keys(), key=lambda x: int(x))
            for int_key, index in enumerate(sorted_keys):
                key = str(int_key)
                if existing_video["votes"][key]:
                    best_option = None
                    best_option_count = -1
                    for option in existing_video["votes"][key]:
                        if existing_video["votes"][key][option] > best_option_count:
                            best_option = option
                            best_option_count = existing_video["votes"][key][option]

                    best_option = best_option.split()
                    best_option = " ".join([word.capitalize()
                                           for word in best_option])

                    returned_speakers[int(index)] = best_option

            print(returned_speakers)

            return {"utterances": existing_video["utterances"], "speakers": returned_speakers}
        else:
            return {"utterances": existing_video["utterances"], "speakers": existing_video["speakers"]}
    options = {
        'format': 'bestaudio[ext=webm]/bestaudio/best',
        'outtmpl': 'videos/%(title)s.%(ext)s',
        'cookiefile': 'cookies.txt'
    }

    with yt_dlp.YoutubeDL(options) as ydl:
        info_dict = ydl.extract_info(url, download=True)
        file_path = ydl.prepare_filename(info_dict)

    aai.settings.api_key = os.getenv("ASSEMBLY_AI_API")
    config = aai.TranscriptionConfig(speaker_labels=True)
    transcriber = aai.Transcriber()
    transcript = transcriber.transcribe(file_path, config=config)

    raw_utterances = [
        {
            "speaker": utterance.speaker,
            "text": utterance.text,
            "start": utterance.start,
            "end": utterance.end
        }
        for utterance in transcript.utterances
    ]

    speakers_set = {utt["speaker"] for utt in raw_utterances}

    # Assign empty label and run through rebuild logic
    for utt in raw_utterances:
        utt["label"] = 0  # temporary placeholder

    separated_data = utterance_seperator.rebuild_utterance_data({
        "utterances": raw_utterances,
        "speakers": list(speakers_set)
    })

    word_index = 0
    for utterance in separated_data["utterances"]:
        utterance_length = len(utterance["text"].split())

        utterance["start"] = transcript.words[word_index].start
        word_index = word_index + utterance_length - 1
        utterance["end"] = transcript.words[word_index].end
        word_index = word_index + 1

    print(separated_data)
    # Call classification APIs for each utterance
    utterance_texts = [utt["text"] for utt in separated_data["utterances"]]
    dialog_preds = call_dialog_classifier_batch(utterance_texts)
    print(dialog_preds)
    sentiment_preds = call_sentiment_classifier_batch(utterance_texts)
    print(sentiment_preds)

    for i, utterance in enumerate(separated_data["utterances"]):
        utterance["label"] = dialog_preds[i]
        utterance["sentiment"] = sentiment_preds[i]

    video_data = {
        "url": url,
        "utterances": separated_data["utterances"],
        "speakers": separated_data["speakers"]
    }

    print(video_data)

    videos_collection.insert_one(video_data)

    try:
        os.remove(file_path)
    except Exception as e:
        print(f"An error occurred: {e}")

    try:
        os.remove("cookies.txt")
    except Exception as e:
        print(f"An error occurred: {e}")

    return {"utterances": separated_data["utterances"], "speakers": separated_data["speakers"]}


def call_dialog_classifier_batch(text_list):
    data = {"inputs": text_list}
    response = requests.post(DIALOG_CLASSIFIER_URL,
                             headers=hf_headers, json=data)

    print("Dialog classifier batch response status:", response.status_code)

    if response.status_code == 200:
        try:
            predictions = response.json()
            if isinstance(predictions, dict) and "predictions" in predictions:
                return [p["label"] for p in predictions["predictions"]]
            elif isinstance(predictions, list) and "label" in predictions[0]:
                return [p["label"] for p in predictions]
            raise ValueError(
                f"Unexpected dialog classifier format: {predictions}")
        except Exception as e:
            print("Error parsing dialog classifier response:", e)
            return ["Miscellaneous"] * len(text_list)
    else:
        print(
            f"Dialog classification failed: {response.status_code}, {response.text}")
        return ["Miscellaneous"] * len(text_list)


def call_sentiment_classifier_batch(text_list):
    data = {"inputs": text_list}
    response = requests.post(SENTIMENT_URL, headers=hf_headers, json=data)

    if response.status_code == 200:
        try:
            predictions = response.json()
            if isinstance(predictions, list) and "label" in predictions[0]:
                return [p["label"] for p in predictions]
            elif isinstance(predictions, dict) and "predictions" in predictions:
                return [p["label"] for p in predictions["predictions"]]
            raise ValueError(
                f"Unexpected sentiment classifier format: {predictions}")
        except Exception as e:
            print("Error parsing sentiment classifier response:", e)
            return ["neutral"] * len(text_list)
    else:
        print(f"Sentiment classification failed: {response.text}")
        return ["neutral"] * len(text_list)
